---------------------------hive体系结构------------------------
Hive的体系结构可以分为以下几部分：
    （1）用户接口主要有三个：CLI，Client 和 WUI。其中最常用的是CLI，Cli启动的时候，会同时启动一个Hive副本。
         Client是Hive的客户端，用户连接至Hive Server。在启动 Client模式的时候，需要指出Hive Server所在节点，并且在该节点启动Hive Server。 WUI是通过浏览器访问Hive。
    （2）Hive将元数据存储在数据库中，如mysql、derby。Hive中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。
    （3）解释器、编译器、优化器完成HQL查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在HDFS中，并在随后有MapReduce调用执行。
    （4）Hive的数据存储在HDFS中，大部分的查询、计算由MapReduce完成（但select * from tbl这样的查询不会生成MapRedcue任务）。
     Hive中包含以下数据模型：Table内部表，External Table外部表，Partition分区，Bucket桶。

Hive的数据模型:
    （1）Hive数据库
    类似传统数据库的DataBase，在第三方数据库里实际是一张表。简单示例命令行 hive > create database test_database;

    （2）内部表
    Hive的内部表与数据库中的Table在概念上是类似。每一个Table在Hive中都有一个相应的目录存储数据。例如一个表pvs，它在HDFS中的路径为/wh/pvs，
    其中wh是在hive-site.xml中由${hive.metastore.warehouse.dir} 指定的数据仓库的目录，所有的Table数据（不包括External Table）都保存在这个目录中。删除表时，元数据与数据都会被删除。
    内部表简单示例：
    创建数据文件：test_inner_table.txt
    创建表：create table test_inner_table (key string)
    加载数据：LOAD DATA LOCAL INPATH ‘filepath’ INTO TABLE test_inner_table
    查看数据：select * from test_inner_table;  select count(*) from test_inner_table
    删除表：drop table test_inner_table

    （3）外部表
    外部表指向已经在HDFS中存在的数据，可以创建Partition。它和内部表在元数据的组织上是相同的，而实际数据的存储则有较大的差异。
    内部表的创建过程和数据加载过程这两个过程可以分别独立完成，也可以在同一个语句中完成，在加载数据的过程中，实际数据会被移动到数据仓库目录中；
    之后对数据对访问将会直接在数据仓库目录中完成。删除表时，表中的数据和元数据将会被同时删除。而外部表只有一个过程，加载数据和创建表同时完成（CREATE EXTERNAL TABLE ……LOCATION），
    实际数据是存储在LOCATION后面指定的 HDFS 路径中，并不会移动到数据仓库目录中。当删除一个External Table时，仅删除该链接。
    外部表简单示例：
    创建数据文件：test_external_table.txt
    创建表：create external table test_external_table (key string)
    加载数据：LOAD DATA INPATH ‘filepath’ INTO TABLE test_inner_table
    查看数据：select * from test_external_table;  •select count(*) from test_external_table
    删除表：drop table test_external_table

    （4）分区
    Partition对应于数据库中的Partition列的密集索引，但是Hive中Partition的组织方式和数据库中的很不相同。
    在Hive中，表中的一个Partition对应于表下的一个目录，所有的Partition的数据都存储在对应的目录中。
    例如pvs表中包含ds和city两个Partition，则对应于ds = 20090801, ctry = US 的HDFS子目录为/wh/pvs/ds=20090801/ctry=US；
    对应于 ds = 20090801, ctry = CA 的HDFS子目录为/wh/pvs/ds=20090801/ctry=CA。
    分区表简单示例：
    创建数据文件：test_partition_table.txt
    创建表：create table test_partition_table (key string) partitioned by (dt string)
    加载数据：LOAD DATA INPATH ‘filepath’ INTO TABLE test_partition_table partition (dt=‘2006’)
    查看数据：select * from test_partition_table;  select count(*) from test_partition_table
    删除表：drop table test_partition_table

    （5）桶
    Buckets是将表的列通过Hash算法进一步分解成不同的文件存储。它对指定列计算hash，根据hash值切分数据，目的是为了并行，每一个Bucket对应一个文件。
    例如将user列分散至32个bucket，首先对user列的值计算hash，对应hash值为0的HDFS目录为/wh/pvs/ds=20090801/ctry=US/part-00000；
    hash值为20的HDFS目录为/wh/pvs/ds=20090801/ctry=US/part-00020。如果想应用很多的Map任务这样是不错的选择。
    桶的简单示例：
    创建数据文件：test_bucket_table.txt
    创建表：create table test_bucket_table (key string) clustered by (key) into 20 buckets
    加载数据：LOAD DATA INPATH ‘filepath’ INTO TABLE test_bucket_table
    查看数据：select * from test_bucket_table;  set hive.enforce.bucketing = true;

    （6）Hive的视图
    视图与传统数据库的视图类似。视图是只读的，它基于的基本表，如果改变，数据增加不会影响视图的呈现；如果删除，会出现问题。如果不指定视图的列，会根据select语句后的生成。
    示例：create view test_view as select * from test

Hive和数据库的异同:
    由于Hive采用了SQL的查询语言HQL，因此很容易将Hive理解为数据库。其实从结构上来看，Hive和数据库除了拥有类似的查询语言，再无类似之处。
    数据库可以用在Online的应用中，但是Hive是为数据仓库而设计的，清楚这一点，有助于从应用角度理解Hive的特性。
Hive和数据库的比较如下表：
                      Hive                          RDBMS
查询语言              HQL                           SQL
数据存储              HDFS                          Raw Device or Local FS
数据格式              用户定义                      系统决定
数据更新              不支持                        支持
索引                  无                            有
执行                  MapReduce                     Executor
执行延迟              高                            低
处理数据规模          大                            小
可扩展性              高                            低
     （1）查询语言。由于 SQL 被广泛的应用在数据仓库中，因此专门针对Hive的特性设计了类SQL的查询语言HQL。熟悉SQL开发的开发者可以很方便的使用Hive进行开发。
     （2）数据存储位置。Hive是建立在Hadoop之上的，所有Hive的数据都是存储在HDFS中的。而数据库则可以将数据保存在块设备或者本地文件系统中。
     （3）数据格式。Hive中没有定义专门的数据格式，数据格式可以由用户指定，用户定义数据格式需要指定三个属性：列分隔符（通常为空格、”\t”、”\x001″）、行分隔符（”\n”）
          以及读取文件数据的方法（Hive中默认有三个文件格式TextFile，SequenceFile以及RCFile）。由于在加载数据的过程中，不需要从用户数据格式到Hive定义的数据格式的转换，因此，
          Hive在加载的过程中不会对数据本身进行任何修改，而只是将数据内容复制或者移动到相应的HDFS目录中。
          而在数据库中，不同的数据库有不同的存储引擎，定义了自己的数据格式。所有数据都会按照一定的组织存储，因此，数据库加载数据的过程会比较耗时。
     （4）数据更新。由于Hive是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive中不支持对数据的改写和添加，所有的数据都是在加载的时候中确定好的。
          而数据库中的数据通常是需要经常进行修改的，因此可以使用INSERT INTO ... VALUES添加数据，使用UPDATE ... SET修改数据。
     （5）索引。之前已经说过，Hive在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些Key建立索引。
          Hive要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。由于MapReduce的引入， Hive可以并行访问数据，
          因此即使没有索引，对于大数据量的访问，Hive仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。
          由于数据的访问延迟较高，决定了Hive不适合在线数据查询。
     （6）执行。Hive中大多数查询的执行是通过Hadoop提供的MapReduce来实现的（类似select * from tbl的查询不需要MapReduce）。而数据库通常有自己的执行引擎。
     （7）执行延迟。Hive在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致Hive执行延迟高的因素是MapReduce框架。由于MapReduce本身具有较高的延迟，
          因此在利用MapReduce执行Hive查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，
          Hive的并行计算显然能体现出优势。
     （8）可扩展性。由于Hive是建立在Hadoop之上的，因此Hive的可扩展性是和Hadoop的可扩展性是一致的。而数据库由于ACID语义的严格限制，扩展行非常有限。
          目前最先进的并行数据库Oracle在理论上的扩展能力也只有100台左右。
     （9）数据规模。由于Hive建立在集群上并可以利用MapReduce进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。

Hive元数据库:
    Hive将元数据存储在RDBMS 中，一般常用的有MYSQL和DERBY。
    启动HIVE的元数据库时，需要进入到hive的安装目录
    启动derby数据库：/home/admin/hive/build/dist/，运行startNetworkServer -h 0.0.0.0。
    连接Derby数据库进行测试：进入derby安装目录：/home/admin/hive/build/dist/db-derby-10.4.1.3-bin/bin
    输入：./ij  Connect 'jdbc:derby://hadoop1:1527/metastore_db;create=true';
    hive元数据对应的表约有20个，其中和表结构信息有关的有9张，其余的10多张或为空，或只有简单的几条记录，以下是部分主要表的简要说明。
    表名 	              说明 	                                                      关联键
    TBLS 	              所有hive表的基本信息 	                                      TBL_ID,SD_ID
    TABLE_PARAM 	      表级属性，如是否外部表，表注释等 	                          TBL_ID
    COLUMNS 	          Hive表字段信息(字段注释，字段名，字段类型，字段序号) 	      SD_ID
    SDS 	              所有hive表、表分区所对应的hdfs数据目录和数据格式 	          SD_ID,SERDE_ID
    SERDE_PARAM 	      序列化反序列化信息，如行分隔符、列分隔符、NULL的表示字符等  SERDE_ID
    PARTITIONS 	          Hive表分区信息 	                                          PART_ID,SD_ID,TBL_ID
    PARTITION_KEYS 	      Hive分区表分区键 	                                          TBL_ID
    PARTITION_KEY_VALS 	  Hive表分区名(键值) 	                                      PART_ID
    从上面表的内容来看，hive整个创建表的过程已经比较清楚了。
    （1）解析用户提交hive语句，对其进行解析，分解为表、字段、分区等hive对象
    （2）根据解析到的信息构建对应的表、字段、分区等对象，从 SEQUENCE_TABLE中获取构建对象的最新ID，与构建对象信息（名称，类型等）一同通过DAO方法写入到元数据表中去，成功后将SEQUENCE_TABLE中对应的最新ID+5。
    实际上常见的RDBMS都是通过这种方法进行组织的，典型的如postgresql，其系统表中和hive元数据一样裸露了这些id信息（oid,cid等），而Oracle等商业化的系统则隐藏了这些具体的ID。
    通过这些元数据可以很容易的读到数据诸如创建一个表的数据字典信息，比如导出建表语名等。

Hive基本操作:
    Create Table语句的一些注意项：
    （1）CREATE TABLE创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用IF NOT EXIST选项来忽略这个异常。
    （2）EXTERNAL 关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（ LOCATION ），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；
         若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。
    （3）LIKE允许用户复制现有的表结构，但是不复制数据。
    （4）用户在建表的时候可以自定义SerDe或者使用自带的 SerDe （ Serialize/Deserilize 的简称，目的是用于序列化和反序列化 ）。
         如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的SerDe。
         在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。
    （5）如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，可以使用STORED AS SEQUENCE或RCFILE或ORCFile。
    （6）有分区的表可以在创建的时候使用 PARTITIONED BY语句。一个表可以拥有一个或者多个分区，每一个分区单独存在一个目录下。
         而且，表和分区都可以对某个列进行CLUSTERED BY操作，将若干个列放入一个桶（bucket）中。也可以利用SORT BY对数据进行排序。这样可以为特定应用提高性能。
    （7）表名和列名不区分大小写，SerDe和属性名区分大小写。表和列的注释是字符串。
    Alter Table语句：主要功能包括Add Partitions, Drop Partitions, Rename Table, Change Column, Add/Replace Columns。
    Create View语句：创建视图。格式CREATE VIEW [IF NOT EXISTS] view_name [ (column_name [COMMENT column_comment], ...) ]
    Showy语句：Show tables;  Show partitions; describe查看表结构。
    Load语句：HIVE装载数据时没有做任何转换，加载到表中的数据只是进入相应的配置单元表的位置。Load操作只是单纯的复制/移动操作，将数据文件移动到Hive表对应的位置。
    Insert语句：插入数据。Hive不支持一条一条的用 insert 语句进行插入操作，这个应该是与hive的storage layer是有关系的，因为它的存储层是HDFS，
    插入一个数据要全表扫描，还不如用整个表的替换来的快些。Hive也不支持update的操作。数据是以load的方式，加载到建立好的表中。数据一旦导入，则不可修改。
    要么drop掉整个表，要么建立新的表，导入新的数据。
    Drop语句：删除一个内部表的同时会同时删除表的元数据和数据。删除一个外部表，只删除元数据而保留数据。
    Limit子句：可以限制查询的记录数。查询的结果是随机选择的。下面的查询语句从 t1 表中随机查询5条记录，SELECT * FROM t1 LIMIT 5。
    正则表达式使用：SELECT语句可以使用正则表达式做列选择，下面的语句查询除了ds和h 之外的所有列：SELECT `(ds|hr)?+.+` FROM sales
    SELECT语句：查询数据。
    Group by, Order by, Sort by子句：聚合可进一步分为多个表，甚至发送到 Hadoop 的 DFS 的文件（可以进行操作，然后使用HDFS的utilitites）。
    可以用hive.map.aggr控制怎么进行汇总。默认为true，配置单元会做的第一级聚合直接在MAP上的任务。这通常提供更好的效率，但可能需要更多的内存来运行成功。
    Join语句：连接操作。一些注意事项：
    （1）Hive只支持等值连接（equality joins）、外连接（outer joins）和（left/right joins）。Hive不支持所有非等值的连接，因为非等值连接非常难转化到map/reduce任务。
    （2）Hive 支持多于2个表的连接。
    （3）join 时，每次 map/reduce 任务的逻辑： reducer 会缓存 join 序列中除了最后一个表的所有表的记录， 再通过最后一个表将结果序列化到文件系统。
         这一实现有助于在reduce端减少内存的使用量。实践中，应该把最大的那个表写在最后（否则会因为缓存浪费大量内存）。
    （4）LEFT，RIGHT 和 FULL OUTER 关键字用于处理 join 中空记录的情况。
    （5）LEFT SEMI JOIN 是 IN/EXISTS 子查询的一种更高效的实现。Hive 当前没有实现 IN/EXISTS 子查询，所以可以用 LEFT SEMI JOIN 重写子查询语句。
         LEFT SEMI JOIN的限制是， JOIN子句中右边的表只能在ON子句中设置过滤条件，在WHERE子句、SELECT子句或其他地方过滤都不行。

HIVE注意项:
    （1）字符集
    Hadoop和Hive都是用UTF-8编码的，所以, 所有中文必须是UTF-8编码, 才能正常使用。
    备注：中文数据load到表里面,，如果字符集不同，很有可能全是乱码需要做转码的，但是hive本身没有函数来做这个。
    （2）压缩
    hive.exec.compress.output 这个参数，默认是false，但是很多时候貌似要单独显式设置一遍，否则会对结果做压缩的，如果这个文件后面还要在hadoop下直接操作，那么就不能压缩了。
    （3）count(distinct)
    当前的Hive不支持在一条查询语句中有多Distinct。如果要在Hive查询语句中实现多Distinct，需要使用至少n+1条查询语句（n为distinct的数目），前n条查询分别对n个列去重，最后一条查询语句对n个去重之后的列做Join操作，得到最终结果。
    （4）JOIN
    只支持等值连接
    （5）DML操作
    只支持INSERT/LOAD操作，无UPDATE和DELTE
    （6）HAVING
    不支持HAVING操作。如果需要这个功能要嵌套一个子查询用where限制
    （7）子查询
    Hive不支持where子句中的子查询
    （8）Join中处理null值的语义区别
    SQL标准中，任何对null的操作（数值比较，字符串操作等）结果都为null。Hive对null值处理的逻辑和标准基本一致，除了Join时的特殊逻辑。
    这里的特殊逻辑指的是，Hive的Join中，作为Join key的字段比较，null=null是有意义的，且返回值为true。
    （9）分号字符
    分号是SQL语句结束标记，在HiveQL中也是，但是在HiveQL中，对分号的识别没有那么智慧，例如：
    select concat(cookie_id,concat(';',’zoo’)) from c02_clickstat_fatdt1 limit 2;
    FAILED: Parse Error: line 0:-1 cannot recognize input '<EOF>' in function specification
    可以推断，Hive解析语句的时候，只要遇到分号就认为语句结束，而无论是否用引号包含起来。
    解决的办法是，使用分号的八进制的ASCII码进行转义，那么上述语句应写成：
    select concat(cookie_id,concat('\073','zoo')) from c02_clickstat_fatdt1 limit 2;
    为什么是八进制ASCII码？我尝试用十六进制的ASCII码，但Hive会将其视为字符串处理并未转义，好像仅支持八进制，原因不详。
    这个规则也适用于其他非SELECT语句，如CREATE TABLE中需要定义分隔符，那么对不可见字符做分隔符就需要用八进制的ASCII码来转义。
    （10）Insert
    根据语法Insert必须加“OVERWRITE”关键字，也就是说每一次插入都是一次重写。
---------------------------------------------------------------